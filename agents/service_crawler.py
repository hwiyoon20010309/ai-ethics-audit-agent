# agents/service_crawler.py
import os
import requests
from bs4 import BeautifulSoup
from langchain_community.tools import TavilySearchResults
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

def crawl_service_info(service_name: str, max_results: int = 3) -> str:
    """
    AI ì„œë¹„ìŠ¤ ì´ë¦„ì„ ì…ë ¥ë°›ì•„ Tavily APIë¡œ ê²€ìƒ‰ í›„, ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìš”ì•½ ë°˜í™˜
    """
    print(f"ğŸŒ '{service_name}' ê´€ë ¨ ì›¹ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

    # 1ï¸âƒ£ Tavily APIë¡œ ê²€ìƒ‰ (langchain community tool)
    search = TavilySearchResults(max_results=max_results)
    results = search.run(f"{service_name} AI ì„œë¹„ìŠ¤ ì„¤ëª… OR ê¸°ìˆ  êµ¬ì¡° OR product overview")

    if not results:
        print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    # 2ï¸âƒ£ ê° í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
    texts = []
    for r in results:
        try:
            url = r["url"]
            res = requests.get(url, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")
            paragraphs = " ".join([p.get_text() for p in soup.find_all("p")])
            texts.append(paragraphs)
            print(f"   ğŸ“„ {url} ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ {r['url']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue

    if not texts:
        return ""

    combined_text = " ".join(texts)[:15000]  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ëƒ„

    # 3ï¸âƒ£ LLM ìš”ì•½
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    prompt = f"""
    ë‹¤ìŒì€ '{service_name}'ì— ëŒ€í•œ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
    ê¸°ìˆ ì  êµ¬ì¡°, ì„œë¹„ìŠ¤ ëª©ì , ì£¼ìš” ê¸°ëŠ¥ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜.
    -----
    {combined_text}
    """
    summary = llm.invoke(prompt)
    print("âœ… ìš”ì•½ ì™„ë£Œ")
    return summary.content
